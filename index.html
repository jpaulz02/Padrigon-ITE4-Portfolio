<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Padrigon ITE 4 Portfolio</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Clarence Taylor</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#prelims">Prelims</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#midterms">Midterms</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Finals</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        John Paul
                        <span class="text-primary">PADRIGON</span>
                    </h1>
                    <div class="subheading mb-5">
                        18 Heizer Street, Balara Filters, Pansol, Quezon City
                        <a href="johnpaul.padrigon@my.jru.edu">johnpaul.padrigon@my.jru.edu</a>
                    </div>
                    <p class="lead mb-5">4th year student of Bachelor of Science in Information Technology at Jose Rizal University</p>
                    <p class="lead mb-5">Welcome! This is my ITE 4 Portfolio</p>
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.facebook.com/"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="https://www.instagram.com/jpaulz02/"><i class="fab fa-facebook-f"></i></a>
                        <a class="social-icon" href="https://x.com/jpaulz02"><i class="fab fa-instagram"></i></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- PRELIMS -->
            <section class="resume-section" id="prelims">
                <div class="resume-section-content">
                    <h2 class="mb-5">Prelim Activities and Exam</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Exercise 2 - NLP Text PreProcessing <div class="subheading mb-3">Grade: 70</div></h3>
                            <span class="text-primary"><a download="Exercise 2 - NLP Text PreProcessing.rar" href="downloads/prelims/Exercise 2 - NLP Text PreProcessing.rar">Exercise 2 - NLP Text PreProcessing Activities</a></span>
                            <img src = "downloads/prelims/e1.png" width="100%">
                            <div class="subheading mb-3">Learning Reflection</div>
                            <p>I can reflect on the activity: I understand much more deeply now how preprocessing works to prepare textual data for a variety of NLP tasks. The way it works is evident from normalization through stop-word removal. Every step showed that these are all very important steps in the processes that will reduce noise while making the data not only manageable but also more accurate in the final analysis.

                                This exercise further proved the point that preprocessing is very far from being a preliminary step; it's what makes an NLP project produce meaningful results. Reflecting on the impact of each preprocessing step, I realized just how much cleaner and uniform the dataset was. This really opened my eyes to seeing how much smaller the size of the dataset is, without losing content-all these in the context of streamlining textual information, with the efficiency it is brought about. Pointing out that pre-processing enhances the dependability and accuracy of the models- a lesson well learned during this session.
                                
                                Moreover, I was equipped with the knowledge on ensuring the integrity of the data set. I realized although it is a pretty easy thing to miss, the selection of preprocessing techniques has real ramifications on the final output fed into downstream tasks, for example, sentiment analysis or even training of the model. In the end, this activity further ingrained within me how much impact a preprocessing pipeline can have and just how important it is to be quite thorough in data preparation with any NLP work moving forward.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Aug 19 at 10:30am - Aug 26 at 10:30am</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Activity 3.1 Implementing Text Representation <div class="subheading mb-3">Grade: 75</div></h3>
                            <span class="text-primary"><a download="Activity 3.1 Implementing Text Representation.rar" href="downloads/prelims/Activity 3.1 Implementing Text Representation.rar">Activity 3.1 Implementing Text Representation Activities</a></span>
                            <img src = "downloads/prelims/e2.png" width="100%">
                            <div class="subheading mb-3">Learning Reflection</div>
                            <p>This activity provided valuable insights into different text representation techniques and their applications in natural language processing. The process highlighted the importance of choosing an appropriate encoding method based on the data and task at hand. For instance, one-hot encoding is simple and intuitive but can become impractical with large vocabularies due to its memory-intensive nature. The Bag-of-Words approach, while efficient, lacks depth since it only captures word counts without accounting for context or word rarity. TF-IDF, on the other hand, showed its utility by emphasizing unique words, making it a powerful tool for tasks that require distinguishing between commonly used and rare terms. Through this activity, we learned that text representation is a foundational aspect of NLP, directly influencing the effectiveness of downstream tasks such as classification and clustering. This experience underscored the need to balance computational efficiency and meaningful representation when dealing with large text datasets.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Aug 29 at 7am - Aug 30 at 10:30am</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelim Asynchronous Activity <div class="subheading mb-3">Grade: 85</div></h3>
                            <span class="text-primary"><a download="Prelim Asynchronous Activity.rar" href="downloads/prelims/Prelim Asynchronous Activity.rar">Prelim Asynchronous Activity</a></span>
                            <img src = "downloads/prelims/e3.png" width="100%">
                            <div class="subheading mb-3">Learning Reflection</div>
                            <p>Handling missing data and input validation have been some of the significant NLP exercises that focused on the importance of thorough preprocessing in workflows. With this exercise 2, I learned to pay attention to missing data since overlooking those introduces noise that disrupts the model performance, for instance, when it made those values convert to string "nan." The use of fillna() function in dealing with those missing values improved the integrity of the data set, and unwanted anomalies were prevented from happening within that dataset. So, this exercise made me realize the critical importance of data cleaning and preparation before performing any transformation in NLP tasks.

                                In Exercise 3, I had a problem of user input validation from users interacting with a word lookup function. Non-numeric inputs could crash the program without input validation which adversely affects user experience. Checking through the isdigit() method was a simple solution for a smoother functionality. The exercise emphasized the need to anticipate errors from the user and assure that applications are designed to offer safety nets that make applications stronger and more user-friendly.
                                
                                In summary, these exercises taught me that the basis for building trustworthy machine learning models lies in proper preprocessing and validation techniques. Whether filling missing data or validating input, these practices improve model quality and guarantee a seamless experience for the user. My awareness of the subtleties of data handling for real-world NLP applications makes me better prepared for all future projects.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Sep 5 at 10:30am - Sep 5 at 12:30pm</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelim Examination SET A - 17<div class="subheading mb-3">Score: 40</div></h3>
                            <img src = "downloads/prelims/pexam.jpg" width="32%">
                            <img src = "downloads/prelims/pexam2.jpg" width="32%">
                            <img src = "downloads/prelims/pexam3.jpg" width="32%">
                            <div class="subheading mb-3">Learning Reflection</div>
                            <p>I had come to appreciate much better core concepts, applications, and real-world relevance of NLP. I had realized that it was the foundational knowledge about the very fact that there exists a difference between the target values and the feature values that made up the basis of training models. Similarly, web scraping and API usage also come as the ways in which data are being extracted and hence become critical for the NLP tasks.

                                It was hard to understand and explain how applications like NER for electronic payments could be useful. This required a level of technical knowledge of how NER actually works as well as an appreciation of the real-world impact. At first, I had a problem describing who the stakeholders are and their roles in such an application. Now, however, I know that banks and the financial world are significantly dependent on these technologies for security and avoiding fraud. Practical application of NER showcased how the broader implications of NLP can be used to solve real world problems in finance.
                                
                                After having all this feedback, I have understood that I should elaborate the roles of the stakeholders better and also give a very vivid explanation of specific roles each of them plays. That would not only help me gain more but would also be beneficial for my communication regarding technical aspects with more fluency. In the future, I will continue to deepen my understanding of how NLP technologies can be applied to real life by studying even more case studies, particularly about how NLP is transforming everyday transactions and interactions.
                                
                                This process of test preparation and reflection has helped me to realize areas for improvement and encourages me to research NLP more and its practical uses outside the classroom.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Sep 12 at 10:30am - Sep 12 at 12:30pm</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Midterms -->
            <section class="resume-section" id="midterms">
                <div class="resume-section-content">
                    <h2 class="mb-5">Midterm Activities and Exam</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Exercise M1 - Implementing NLP Application Models using Spacy <div class="subheading mb-3">Grade: 85</div></h3>
                            <span class="text-primary"><a download="Exercise M1 - Implementing NLP Application Models using Spacy.rar" href="downloads/midterms/Exercise M1 - Implementing NLP Application Models using Spacy.rar">Exercise M1 - Implementing NLP Application Models using Spacy Activities</a></span>
                            <img src = "downloads/midterms/e1.png" width="100%">
                            <div class="subheading mb-3">Learning Reflection</div>
                            <p>This activity helps me reflect on some key aspects of the NLP tasks using the spaCy library, including text classification, entity recognition, POS tagging, sentiment analysis, and text summarization. Each task allowed me to apply the core NLP concepts by hands-on practice that helps deepen my understanding of the ways language models interpret and classify text. The whole process would include developing the spam and ham classifier which would learn me how a dataset would be created from nothing; how data should be marked in terms of assigning classification labels, and that an iterative model is trained toward distinguishing patterns. Named entity recognition and POS tagging, both, brought me into contact with spaCy's pre-trained models. Briefly put, I can get names and places very easily through NER or even parts of speech tagging using POS tagging. Firsthand, I was amazed at how these foundational NLP tasks would offer not only structural insights but also contextual information into the language necessary to understand the syntax of a sentence and the information it categorizes.

                                The task of sentiment analysis really brought home the importance of good training data and methods, including dropout regularization to avoid overfitting, when one trained a model to classify text as "POSITIVE" or "NEGATIVE". Witnessing how the model learned to identify subtle emotional nuances in text gave me a very practical appreciation for the challenges in sentiment analysis. Finally, in the simple text summarizer, I experimented with a very simple yet effective technique of sentence selection based on token frequency, allowing me to capture the crux of larger texts. This is useful for short text summaries, but it has limitations for more complex documents, so advanced summarization techniques become necessary in those cases. In a nutshell, through this activity, I come to understand more about how spaCy is capable in its execution and overall structured way to train any model on various NLP tasks from classification all the way to summarization.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Sep 16 at 12pm - Sep 20 at 10:30am</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Exercise M1.1 - Evaluating NLP Model Performance <div class="subheading mb-3">Grade: 70</div></h3>
                            <span class="text-primary"><a download="Exercise M1.1 - Evaluating NLP Model Performance.rar" href="downloads/midterms/Exercise M1.1 - Evaluating NLP Model Performance.rar">Exercise M1.1 - Evaluating NLP Model Performance Activities</a></span>
                            <img src = "downloads/midterms/e2.png" width="100%">
                            <div class="subheading mb-3">Learning Reflection</div>
                            <p>This analysis makes me reflective about the size of the dataset, quality, and its significance in affecting the performance of NLP models on numerous tasks and reiterating the requirement for task-oriented preprocessing and data management procedures. Results from each type of task like classification, Named Entity Recognition (NER), Part of Speech (POS) tagging, sentiment analysis, and summary text present a scenario of need for proper balance between dataset sizes and the application of adequate preprocessing procedures for optimizing performance.

                                For instance, in spam detection through text classification, I found that although the size of the dataset did increase accuracy, good preprocessing was critical in minimizing false positives. High recall with poor precision on the small, low-quality dataset was seen, where it became apparent that minimal preprocessing might preserve unnecessary noise that negatively impacts accuracy. On the contrary, the NER task showed a clear weakness: over-preprocessing, like stopping words, can severely degrade model performance by deleting context that is important to identify subtle entities. Such an outcome reveals that it is possible that over-processing can actually reduce the performance of tasks that are heavily dependent on subtle contextual hints.
                                
                                The POS task did not show such sensitivity with respect to data size or quality of preprocessing and could achieve near-perfect results in all conditions. This would imply that even simpler tasks like part-of-speech tagging require minimal preprocessing and work reasonably well with small datasets. The results for the sentiment analysis, however, indicate that small datasets were reasonable even though larger dataset size coupled with low-level preprocessing helped maintain high accuracy, and capturing more data diversity did not necessarily come at the cost of excessive cleaning.
                                
                                In text summarization, I learned that in the case of ROUGE, scores were found to relate well with both size of dataset and quality of preprocessing. Good-quality summarization came out with higher sizes of datasets, but on the other hand, when it comes to summarizing a smaller-sized dataset having a poor quality in terms of preprocessing, nothing really valuable can be pulled out from the dataset, therefore underlining the very process that summarization is; key content synthesis, thus sensitive both by the size and quality of dataset.
                                
                                In summary, this exercise improved my perception of how performance varies wildly among NLP models given variations in the size of datasets and rigor in preprocessing, emphasizing that each such task needs its unique setting to maximize the outcomes.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Sep 19 at 10:30am - Sep 28 at 10:30am</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Exercise 5 - (BERT-based QnA System) <div class="subheading mb-3">Grade: 80</div></h3>
                            <span class="text-primary"><a download="Exercise 5 - (BERT-based QnA System).rar" href="downloads/midterms/Exercise 5 - (BERT-based QnA System).rar">Exercise 5 - (BERT-based QnA System) Activities</a></span>
                            <img src = "downloads/midterms/e3.png" width="100%">
                            <div class="subheading mb-3">Learning Reflection</div>
                            <p>This exercise taught me better ways to fine-tune and evaluate pre-trained BERT models in Q&A tasks. For this experiment, I took four different BERT-based models with different configurations and architectures and tested them against a set of manually constructed questions, so I was able to see how the model choice affects performance in terms of token-level accuracy and semantic understanding. By looking at models such as deepset/bert-base-cased-squad2 and google-bert-large-uncased-whole-word-masking-finetuned-squad, it could be seen that it had the exact token matching place where specific models could show better semantic meaning, thus having that reflected in measures of Precision, Recall, F1-score, BERTScore, etc. But one of the principal inferences I made there is that structure and training objectives were affecting suitability of an application in different models. The use case where it performed better would include token-level precision and accuracy, such as in use cases where answers are a matter of critical importance or even legal/financial related. On the other hand, google-bert-large-uncased-whole-word-masking-finetuned-squad exhibited strong strength in BERTScore on semantic similarity. This use case would fit applications relying on the contextual understanding within customer service chatbots or when context is required.

                                I also saw that salti/bert-base-multilingual-cased-finetuned-squad was balanced in terms of token-level accuracy and semantic understanding, especially in multilingual contexts, making it highly versatile for different Q&A scenarios. This exercise particularly brought home the importance of choosing models based on requirements for specific tasks, mainly when balancing exact match accuracy against a broader, context-sensitive understanding.
                                
                                In summary, this experiment further reiterates that fine-tuning pre-trained BERT models for Q&A tasks requires a careful selection process based on the desired outcome. It also pointed out the value of metrics such as BERTScore that capture semantic similarity as complementary to traditional token-level metrics in providing a holistic view of the model. This reflection has proven informative on how the diverse configurations of models and various performance evaluation metrics would go into affecting Q&A systems in being more functional towards actual application.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Oct 3 at 12am - Oct 7 at 11:59pm</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">P-M1 (Project Proposal Presentation) <div class="subheading mb-3">Grade: 80</div></h3>
                            <span class="text-primary"><a download="P-M1 (Project Proposal Presentation).rar" href="downloads/midterms/P-M1 (Project Proposal Presentation).rar">P-M1 (Project Proposal Presentation)</a></span>
                            <img src = "downloads/midterms/e4.png" width="100%">
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Oct 7 at 10:30am - Oct 14 at 10:30am</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Midterm Examination <div class="subheading mb-3">Score: N/A</div></h3>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Sep 12 at 10:30am - Sep 12 at 12:30pm</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
